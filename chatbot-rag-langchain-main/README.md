# ðŸ§  Chatbot Using RAG and LangChain

A Streamlit-based chatbot powered by Retrieval-Augmented Generation (RAG) and OpenAI. Upload your PDFs and chat with them! This app leverages LangChain, FAISS, and OpenAIâ€™s GPT models to extract and query document content with metadata-aware answers.
[App Screenshot](thumbnail.webp)
---
## ðŸ”§ Features
* ðŸ” **Upload multiple PDFs** and query across all of them
* ðŸ“„ **Metadata-rich answers** with filename and page references
* ðŸ§  Uses **LangChain + FAISS** for semantic search
* ðŸ¤– **Streamlit Chat UI** for natural conversation
* ðŸ’¾ **OpenAI API support** with streaming responses

---

## ðŸ“ Project Structure
```
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md             # â† You're reading it
â”œâ”€â”€ app.py                # Main Streamlit app
â”œâ”€â”€ brain.py              # PDF parsing and vector index logic
â”œâ”€â”€ compare medium.gif    # Optional UI illustration
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ thumbnail.webp        # Preview image
---
## ðŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/shamazhooda/chatbot-rag-langchain.git
cd chatbot-rag-langchain
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Set OpenAI API Key

Create a `.streamlit/secrets.toml` file with:

```toml
OPENAI_API_KEY = "your-openai-key"
```

Or export it via environment variable:

```bash
export OPENAI_API_KEY="your-openai-key"
```

### 4. Run the App

```bash
streamlit run app.py
```

---

## ðŸ“š How It Works

1. **Upload PDFs** via the UI
2. Each PDF is parsed using `PyPDF2` and chunked via LangChainâ€™s `RecursiveCharacterTextSplitter`
3. Chunks are embedded using OpenAI Embeddings
4. Stored in a FAISS vector store for semantic similarity search
5. Queries are matched to top PDF chunks and passed to ChatGPT with context
6. Answers include **file name** and **page number** metadata for citation

---

## ðŸ› ï¸ Tech Stack

* [Streamlit](https://streamlit.io/) â€“ UI framework
* [LangChain](https://www.langchain.com/) â€“ PDF chunking and retrieval
* [FAISS](https://github.com/facebookresearch/faiss) â€“ Vector search backend
* [OpenAI GPT](https://platform.openai.com/docs) â€“ LLM-based answer generation
* [PyPDF2](https://pypi.org/project/pypdf/) â€“ PDF parsing

---

## âœ… Example Prompt

> "What are the main points from the introduction?"

> Answer: The introduction highlights... **(example.pdf, page 1)**

---

## Architecture and Storage

- UI: `app.py` (Streamlit)
- RAG pipeline: `brain.py` (parse â†’ chunk â†’ embed â†’ index)
- Model: OpenAI Chat Completions API (streaming)
- Retriever: FAISS similarity search

### Storage
- PDFs: in-memory during session (not persisted by default).
- Chunks/metadata: in-memory `Document` objects.
- Vector store: FAISS, created in-memory and cached via `st.cache_resource`. Lives for the server process lifetime; cleared on restart or cache clear.
- Chat history: `st.session_state` (per browser session).

### Optional persistence
- To keep vectors across restarts, persist FAISS:

```python
# brain.py
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS

def save_index_local(vectordb: FAISS, path: str):
    vectordb.save_local(path)

def load_index_local(path: str, openai_api_key: str) -> FAISS:
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)
```

```python
# app.py (around create_vectordb)
DB_DIR = "data/faiss_index"
if os.path.isdir(DB_DIR):
    vectordb = load_index_local(DB_DIR, OPENAI_API_KEY)
else:
    vectordb = get_index_for_pdf([f.getvalue() for f in files], filenames, OPENAI_API_KEY)
    save_index_local(vectordb, DB_DIR)
```

### What each module does

- `app.py`
  - Loads `OPENAI_API_KEY` from `st.secrets` or environment.
  - Uploads PDFs with `st.file_uploader`.
  - Builds or retrieves the FAISS index using `@st.cache_resource` in `create_vectordb(...)`.
  - On a question:
    - Retrieves top-k chunks with `vectordb.similarity_search(...)`.
    - Injects those chunks into a system prompt.
    - Streams model output using OpenAIâ€™s v1 Chat Completions API.

- `brain.py`
  - `parse_pdf(...)`: uses `pypdf` to extract text from each page and normalizes whitespace/hyphenation.
  - `text_to_docs(...)`: splits text into chunks using `RecursiveCharacterTextSplitter`, attaches `filename`, `page`, `chunk` metadata.
  - `docs_to_index(...)`: creates a FAISS index via `FAISS.from_documents(...)` with `OpenAIEmbeddings`.
  - `get_index_for_pdf(...)`: orchestrates PDF parse â†’ chunk â†’ embed â†’ FAISS index.

### Data flow of a query

1. Upload PDFs â†’ `parse_pdf` extracts text per page.
2. Text â†’ `text_to_docs` creates chunked `Document` objects with metadata.
3. Docs â†’ `docs_to_index` embeds with `OpenAIEmbeddings` and builds a FAISS index.
4. On a user question â†’ `similarity_search(k=3)` returns the most relevant chunks.
5. App forms a system prompt with those chunks and streams a response from the model.
6. The UI displays tokens as they arrive.

### Security and limits

- Your OpenAI key is required; store it in `.streamlit/secrets.toml` or environment.
- Uploaded files stay in memory and are not written to disk unless you add persistence.
- FAISS index is in-memory unless you add the optional save/load shown above.

### Updated README content you can paste

```markdown
<code_block_to_apply_changes_from>
```

### Run
- Set `OPENAI_API_KEY` in `.streamlit/secrets.toml` or as an env var.
- `pip install -r requirements.txt`
- `streamlit run app.py`
```

- Want me to apply these README edits and add the optional persistence helpers to `brain.py` for you?